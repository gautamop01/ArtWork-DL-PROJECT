{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shash\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import io\n",
    "from scipy.sparse import csr_matrix\n",
    "import faiss\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"Artificio/WikiArt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = ds['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "artist_encoder = LabelEncoder()\n",
    "style_encoder = LabelEncoder()\n",
    "genre_encoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LabelEncoder()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;LabelEncoder<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.preprocessing.LabelEncoder.html\">?<span>Documentation for LabelEncoder</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>LabelEncoder()</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "artist_encoder.fit(train_data['artist'])\n",
    "style_encoder.fit(train_data['style'])\n",
    "genre_encoder.fit(train_data['genre'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_metadata(example):\n",
    "    example['artist_encoded'] = artist_encoder.transform([example['artist']])[0]\n",
    "    example['style_encoded'] = style_encoder.transform([example['style']])[0]\n",
    "    example['genre_encoded'] = genre_encoder.transform([example['genre']])[0]\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_encoded = train_data.map(encode_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_clip_embeddings(example):\n",
    "    if isinstance(example['image'], str):  # It's a file path\n",
    "        image = Image.open(example['image']).convert(\"RGB\")\n",
    "    elif isinstance(example['image'], Image.Image):  # It's a PIL image\n",
    "        image = example['image'].convert(\"RGB\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected type for 'image': {type(example['image'])}\")\n",
    "\n",
    "    inputs = clip_processor(images=image, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        image_features = clip_model.get_image_features(**inputs)\n",
    "    \n",
    "    example['image_embeddings'] = image_features.cpu().numpy()\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset_encoded' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[77], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m dataset_encoded \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_encoded\u001b[49m\u001b[38;5;241m.\u001b[39mmap(generate_clip_embeddings)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataset_encoded' is not defined"
     ]
    }
   ],
   "source": [
    "dataset_encoded = dataset_encoded.map(generate_clip_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_embeddings(example):\n",
    "    # Ensure metadata is properly processed\n",
    "    metadata_vector = np.array([\n",
    "        example['artist_encoded'],\n",
    "        example['style_encoded'],\n",
    "        example['genre_encoded']\n",
    "    ], dtype=np.float32)\n",
    "\n",
    "    # Normalize metadata vector\n",
    "    metadata_vector = torch.nn.functional.normalize(\n",
    "        torch.tensor(metadata_vector), dim=0\n",
    "    ).numpy()\n",
    "\n",
    "    # Handle the case where image embeddings are stored as a list\n",
    "    image_embeddings = np.array(example['image_embeddings'])  # Convert to NumPy array if needed\n",
    "\n",
    "    # Combine embeddings\n",
    "    combined_embedding = np.concatenate([image_embeddings.flatten(), metadata_vector])\n",
    "    example['combined_embeddings'] = combined_embedding\n",
    "    return example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 103250/103250 [01:08<00:00, 1505.15 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_encoded = dataset_encoded.map(combine_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings prepared and saved successfully!\n"
     ]
    }
   ],
   "source": [
    "np.save(\"combined_embeddings.npy\", np.vstack(dataset_encoded['combined_embeddings']))\n",
    "\n",
    "print(\"Embeddings prepared and saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embeddings = np.array([example['combined_embeddings'] for example in dataset_encoded])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embeddings = np.load('combined_embeddings.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embeddings = all_embeddings / np.linalg.norm(all_embeddings, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content-Based Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_embeddings_for_recommendation(current_embedding, previous_embedding=None, weight=0.7):\n",
    "    \"\"\"\n",
    "    Combine embeddings using a weighted approach.\n",
    "    \n",
    "    Args:\n",
    "        current_embedding (np.array): Embedding of the current artwork.\n",
    "        previous_embedding (np.array): Combined embedding from previous interactions, or None.\n",
    "        weight (float): Weight given to the current embedding (0 <= weight <= 1).\n",
    "    \n",
    "    Returns:\n",
    "        np.array: Weighted combined embedding.\n",
    "    \"\"\"\n",
    "    if previous_embedding is None:\n",
    "        # No previous embedding exists\n",
    "        return current_embedding\n",
    "    else:\n",
    "        # Combine embeddings using weights\n",
    "        return weight * current_embedding + (1 - weight) * previous_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_similar_artworks(combined_embedding, all_embeddings, k=5):\n",
    "    \"\"\"\n",
    "    Recommend artworks similar to the combined embedding.\n",
    "    \n",
    "    Args:\n",
    "        combined_embedding (np.array): Weighted combined embedding for recommendation.\n",
    "        all_embeddings (np.array): All artwork embeddings in the dataset.\n",
    "        k (int): Number of recommendations to return.\n",
    "    \n",
    "    Returns:\n",
    "        list: Indices of the top-k recommended artworks.\n",
    "    \"\"\"\n",
    "    # Compute cosine similarity\n",
    "    similarities = cosine_similarity([combined_embedding], all_embeddings)\n",
    "    top_k_indices = similarities.argsort()[0][-k:][::-1]  # Top-k most similar\n",
    "    return top_k_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended Artwork Indices: [   10 60455 97505 27393 75978]\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "# Assume `current_embedding` is the embedding of the clicked artwork\n",
    "# `previous_combined_embedding` is the embedding combined from prior interactions (initially None)\n",
    "# `all_embeddings` contains all the artwork embeddings in the dataset\n",
    "\n",
    "# User clicks on an artwork\n",
    "current_embedding = all_embeddings[10]  # Example: clicked artwork's embedding\n",
    "previous_combined_embedding = None  # Initially, no previous embedding exists\n",
    "\n",
    "# Combine embeddings\n",
    "weight = 0.7  # Example: give 70% weight to the current click\n",
    "combined_embedding = combine_embeddings_for_recommendation(\n",
    "    current_embedding, previous_combined_embedding, weight\n",
    ")\n",
    "\n",
    "# Update the previous combined embedding for future interactions\n",
    "previous_combined_embedding = combined_embedding\n",
    "\n",
    "# Recommend similar artworks\n",
    "recommended_indices = recommend_similar_artworks(combined_embedding, all_embeddings)\n",
    "print(\"Recommended Artwork Indices:\", recommended_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the first data point from the training set\n",
    "current_data_point = ds['train'][10]\n",
    "\n",
    "# Check the column name that contains the image\n",
    "image_column_name = \"image\"  # Replace with the actual column name if different\n",
    "\n",
    "# Get the image from the dataset\n",
    "image_data = current_data_point[image_column_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the image\n",
    "if isinstance(image_data, Image.Image):  # For already decoded images\n",
    "    image_data.show()\n",
    "elif isinstance(image_data, bytes):  # For encoded images (e.g., byte strings)\n",
    "    image = Image.open(io.BytesIO(image_data))\n",
    "    image.show()\n",
    "else:\n",
    "    print(\"Image format not recognized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in recommended_indices:\n",
    "    curr_img_data = ds['train'][int(i)]['image']\n",
    "    if isinstance(image_data, Image.Image):  # For already decoded images\n",
    "        curr_img_data.show()\n",
    "    elif isinstance(image_data, bytes):  # For encoded images (e.g., byte strings)\n",
    "        new_image = Image.open(io.BytesIO(curr_img_data))\n",
    "        new_image.show()\n",
    "    else:\n",
    "        print(\"Image format not recognized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_interactions = [\n",
    "    [0, 1],  # User 0 interacted with data point 1\n",
    "    [0, 2],  # User 0 interacted with data point 2\n",
    "    [1, 1],  # User 1 interacted with data point 0\n",
    "    [1, 2],  # User 1 interacted with data point 3\n",
    "    [2, 1],  # User 2 interacted with data point 4\n",
    "    [3, 100],\n",
    "    [4, 100],\n",
    "    [3, 101],\n",
    "    [4, 101],\n",
    "    [3, 102]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ids = [interaction[0] for interaction in user_interactions]\n",
    "data_point_indices = [interaction[1] for interaction in user_interactions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_users = max(user_ids) + 1  # Total number of users\n",
    "n_items = all_embeddings.shape[0]  # Total number of data points from embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [1] * len(user_interactions)  # Interaction weights (all set to 1 here)\n",
    "interaction_matrix = csr_matrix((data, (user_ids, data_point_indices)), shape=(n_users, n_items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse Interaction Matrix Shape: (5, 103250)\n",
      "Non-zero interactions: 10\n",
      "User Similarity Matrix:\n",
      "[[1.         1.         0.70710678 0.         0.        ]\n",
      " [1.         1.         0.70710678 0.         0.        ]\n",
      " [0.70710678 0.70710678 1.         0.         0.        ]\n",
      " [0.         0.         0.         1.         0.81649658]\n",
      " [0.         0.         0.         0.81649658 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Sparse Interaction Matrix Shape: {interaction_matrix.shape}\")\n",
    "print(f\"Non-zero interactions: {interaction_matrix.nnz}\")\n",
    "\n",
    "# Compute user-user similarity using cosine similarity\n",
    "user_similarity = cosine_similarity(interaction_matrix)\n",
    "\n",
    "print(\"User Similarity Matrix:\")\n",
    "print(user_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def recommend_items_user_user(user_id, interaction_matrix, user_similarity, top_k=5):\n",
    "    \"\"\"\n",
    "    Recommend items to a user based on user-user collaborative filtering.\n",
    "\n",
    "    Args:\n",
    "        user_id: ID of the user to recommend items for.\n",
    "        interaction_matrix: Sparse matrix of user-item interactions.\n",
    "        user_similarity: User similarity matrix.\n",
    "        top_k: Number of recommendations to generate.\n",
    "\n",
    "    Returns:\n",
    "        List of recommended item indices.\n",
    "    \"\"\"\n",
    "    # Get user similarity scores for the given user\n",
    "    similar_users = np.argsort(-user_similarity[user_id])[1:]  # Exclude self (at index 0)\n",
    "\n",
    "    # Get indices of items interacted by the target user\n",
    "    target_user_items = set(interaction_matrix[user_id].nonzero()[1])\n",
    "\n",
    "    # Keep track of item scores\n",
    "    item_scores = {}\n",
    "\n",
    "    # Aggregate scores from similar users\n",
    "    for similar_user in similar_users:\n",
    "        similarity_score = user_similarity[user_id, similar_user]\n",
    "\n",
    "        # Get items interacted by the similar user\n",
    "        similar_user_items = interaction_matrix[similar_user].nonzero()[1]\n",
    "\n",
    "        for item in similar_user_items:\n",
    "            if item not in target_user_items:  # Exclude already interacted items\n",
    "                if item not in item_scores:\n",
    "                    item_scores[item] = 0\n",
    "                item_scores[item] += similarity_score\n",
    "\n",
    "    # Sort items by aggregated score and return top_k recommendations\n",
    "    recommended_items = sorted(item_scores.keys(), key=lambda x: -item_scores[x])[:top_k]\n",
    "    return recommended_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended items for user 2: [2, 100, 101]\n"
     ]
    }
   ],
   "source": [
    "user_id = 2  # Example user ID\n",
    "recommended_items = recommend_items_user_user(user_id, interaction_matrix, user_similarity, top_k=3)\n",
    "print(f\"Recommended items for user {user_id}: {recommended_items}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative Filtering using NCF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArtworkDataset():\n",
    "    def __init__(self, user_item_matrix, item_embeddings):\n",
    "        \"\"\"\n",
    "        user_item_matrix: List of tuples [(user_id, item_id), ...]\n",
    "        item_embeddings: Tensor of shape (num_items, combined_embedding_dim) -> Precomputed combined embeddings\n",
    "        \"\"\"\n",
    "        self.user_item_matrix = user_item_matrix\n",
    "        self.item_embeddings = item_embeddings\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.user_item_matrix)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        user_id, item_id = self.user_item_matrix[idx]\n",
    "        item_embedding = self.item_embeddings[item_id]\n",
    "        return user_id, item_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NCFModel(nn.Module):\n",
    "    def __init__(self, num_users, embedding_dim, combined_embedding_dim, hidden_layers):\n",
    "        super(NCFModel, self).__init__()\n",
    "        # Embedding layer for users (initialize with max number of users)\n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
    "        \n",
    "        # MLP for interaction modeling\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embedding_dim + combined_embedding_dim, hidden_layers[0]),\n",
    "            nn.ReLU(),\n",
    "            *[layer for hidden_dim in zip(hidden_layers[:-1], hidden_layers[1:]) \n",
    "              for layer in (nn.Linear(hidden_dim[0], hidden_dim[1]), nn.ReLU())]\n",
    "        )\n",
    "        \n",
    "        # Output layer\n",
    "        self.output = nn.Linear(hidden_layers[-1], 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, user_id, item_embedding):\n",
    "        user_vec = self.user_embedding(user_id)\n",
    "        combined = torch.cat([user_vec, item_embedding], dim=-1)\n",
    "        hidden = self.mlp(combined)\n",
    "        return self.sigmoid(self.output(hidden))\n",
    "    \n",
    "    def resize_user_embeddings(self, new_num_users):\n",
    "        \"\"\"\n",
    "        Resize the user embedding layer to accommodate new users.\n",
    "        \"\"\"\n",
    "        old_weights = self.user_embedding.weight.data\n",
    "        self.user_embedding = nn.Embedding(new_num_users, old_weights.size(1))\n",
    "        with torch.no_grad():\n",
    "            self.user_embedding.weight[:old_weights.size(0)] = old_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_users(user_item_matrix):\n",
    "    return max(user_id for user_id, _ in user_item_matrix) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 64\n",
    "hidden_layers = [128, 64, 32]\n",
    "batch_size = 128\n",
    "learning_rate = 0.001\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_matrix = [\n",
    "    [0, 1],  # User 0 interacted with data point 1\n",
    "    [0, 2],  # User 0 interacted with data point 2\n",
    "    [1, 1],  # User 1 interacted with data point 0\n",
    "    [1, 2],  # User 1 interacted with data point 3\n",
    "    [2, 1],  # User 2 interacted with data point 4\n",
    "    [3, 100],\n",
    "    [4, 100],\n",
    "    [3, 101],\n",
    "    [4, 101],\n",
    "    [4, 10]\n",
    "]\n",
    "\n",
    "all_embeddings_ncf = torch.tensor(all_embeddings, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_users = get_num_users(user_item_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ncf = ArtworkDataset(user_item_matrix, all_embeddings_ncf)\n",
    "data_loader = DataLoader(dataset_ncf, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NCFModel(num_users, embedding_dim, all_embeddings_ncf.size(1), hidden_layers)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.7312\n",
      "Epoch 2/10, Loss: 0.7181\n",
      "Epoch 3/10, Loss: 0.7096\n",
      "Epoch 4/10, Loss: 0.7037\n",
      "Epoch 5/10, Loss: 0.6980\n",
      "Epoch 6/10, Loss: 0.6925\n",
      "Epoch 7/10, Loss: 0.6872\n",
      "Epoch 8/10, Loss: 0.6816\n",
      "Epoch 9/10, Loss: 0.6760\n",
      "Epoch 10/10, Loss: 0.6698\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for user_id, item_embedding in data_loader:\n",
    "        user_id = user_id.long()  # User IDs\n",
    "        preds = model(user_id, item_embedding.float())\n",
    "        labels = torch.ones_like(preds)  # Replace with actual labels if available\n",
    "        loss = criterion(preds, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommendation score: 0.519916296005249\n"
     ]
    }
   ],
   "source": [
    "new_user_id = 3\n",
    "if new_user_id >= model.user_embedding.num_embeddings:\n",
    "    print(\"Resizing user embedding for new users.\")\n",
    "    model.resize_user_embeddings(new_user_id + 1)\n",
    "\n",
    "# Inference Example\n",
    "user_id = torch.tensor([0])  # Example user\n",
    "item_embedding = all_embeddings_ncf[1]  # Example item embedding\n",
    "prediction = model(user_id, item_embedding.unsqueeze(0))\n",
    "print(f\"Recommendation score: {prediction.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_k_recommendations(user_id, model, all_embeddings, k=5):\n",
    "    \"\"\"\n",
    "    user_id: The user for whom we want to get the recommendations\n",
    "    model: The trained NCF model\n",
    "    all_embeddings: The precomputed item embeddings (tensor)\n",
    "    k: The number of recommendations to return (default 5)\n",
    "    \n",
    "    Returns:\n",
    "    top_k_items_with_scores: List of tuples [(item_id, score), ...] for the top k recommended items\n",
    "    \"\"\"\n",
    "    # Ensure user_id is a tensor\n",
    "    user_id = torch.tensor([user_id], dtype=torch.long)\n",
    "    \n",
    "    # Get the user embedding from the model\n",
    "    user_embedding = model.user_embedding(user_id)\n",
    "    \n",
    "    # Compute predicted scores for all items\n",
    "    with torch.no_grad():\n",
    "        scores = []\n",
    "        for item_id in range(all_embeddings.shape[0]):  # Loop over all items\n",
    "            item_embedding = all_embeddings[item_id].unsqueeze(0)  # Get embedding for the item\n",
    "            score = model(user_id, item_embedding)  # Predict score for this item\n",
    "            scores.append(score.item())  # Store the score\n",
    "        \n",
    "    # Convert scores to tensor\n",
    "    scores = torch.tensor(scores)\n",
    "    \n",
    "    # Get the top k item indices and scores\n",
    "    top_k_values, top_k_indices = torch.topk(scores, k)\n",
    "    \n",
    "    # Combine indices and their scores into a list of tuples\n",
    "    top_k_items_with_scores = [(top_k_indices[i].item(), top_k_values[i].item()) for i in range(k)]\n",
    "    \n",
    "    return top_k_items_with_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 recommendations for user 3:\n",
      "Item ID: 100, Predicted Score: 0.5171\n",
      "Item ID: 83126, Predicted Score: 0.5170\n",
      "Item ID: 67353, Predicted Score: 0.5170\n",
      "Item ID: 102429, Predicted Score: 0.5170\n",
      "Item ID: 2480, Predicted Score: 0.5169\n"
     ]
    }
   ],
   "source": [
    "user_id = 3  # Example user ID\n",
    "top_k_items_with_scores = get_top_k_recommendations(user_id, model, all_embeddings_ncf, k=5)\n",
    "\n",
    "# Print the top 5 items along with their predicted scores\n",
    "print(f\"Top 5 recommendations for user {user_id}:\")\n",
    "for item_id, score in top_k_items_with_scores:\n",
    "    print(f\"Item ID: {item_id}, Predicted Score: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, j in top_k_items_with_scores:\n",
    "    curr_img_data = ds['train'][int(i)]['image']\n",
    "    if isinstance(image_data, Image.Image):  # For already decoded images\n",
    "        curr_img_data.show()\n",
    "    elif isinstance(image_data, bytes):  # For encoded images (e.g., byte strings)\n",
    "        new_image = Image.open(io.BytesIO(curr_img_data))\n",
    "        new_image.show()\n",
    "    else:\n",
    "        print(\"Image format not recognized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid Model using Content-Based filtering and 1st Collaborative Filtering approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_recommendation(user_id, combined_embedding, all_embeddings, interaction_matrix, user_similarity, content_weight=0.6, collaborative_weight=0.4, top_k=5):\n",
    "    \"\"\"\n",
    "    Hybrid recommendation system combining content-based and collaborative filtering.\n",
    "\n",
    "    Args:\n",
    "        user_id: ID of the user to recommend items for.\n",
    "        combined_embedding: Weighted combined embedding for content-based recommendation.\n",
    "        all_embeddings: All artwork embeddings in the dataset.\n",
    "        interaction_matrix: Sparse matrix of user-item interactions.\n",
    "        user_similarity: User similarity matrix.\n",
    "        content_weight: Weight for content-based recommendations.\n",
    "        collaborative_weight: Weight for collaborative recommendations.\n",
    "        top_k: Number of recommendations to generate.\n",
    "\n",
    "    Returns:\n",
    "        List of recommended item indices.\n",
    "    \"\"\"\n",
    "    # Content-based recommendations\n",
    "    content_similarities = cosine_similarity([combined_embedding], all_embeddings)\n",
    "    content_scores = content_similarities[0]\n",
    "\n",
    "    # Collaborative filtering recommendations\n",
    "    similar_users = np.argsort(-user_similarity[user_id])[1:]  # Exclude self (at index 0)\n",
    "    target_user_items = set(interaction_matrix[user_id].nonzero()[1])\n",
    "    collaborative_scores = np.zeros(all_embeddings.shape[0])\n",
    "\n",
    "    # Aggregate scores from similar users\n",
    "    for similar_user in similar_users:\n",
    "        similarity_score = user_similarity[user_id, similar_user]\n",
    "        similar_user_items = interaction_matrix[similar_user].nonzero()[1]\n",
    "\n",
    "        for item in similar_user_items:\n",
    "            if item not in target_user_items:  # Exclude already interacted items\n",
    "                collaborative_scores[item] += similarity_score\n",
    "\n",
    "    # Normalize both scores\n",
    "    content_scores = content_scores / np.max(content_scores) if np.max(content_scores) > 0 else content_scores\n",
    "    collaborative_scores = collaborative_scores / np.max(collaborative_scores) if np.max(collaborative_scores) > 0 else collaborative_scores\n",
    "\n",
    "    # Combine scores using the specified weights\n",
    "    final_scores = content_weight * content_scores + collaborative_weight * collaborative_scores\n",
    "\n",
    "    # Get top-k recommendations\n",
    "    recommended_items = np.argsort(-final_scores)[:top_k]\n",
    "    return recommended_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended items for user 2 (hybrid): [    2    10 60455 97505 27393]\n"
     ]
    }
   ],
   "source": [
    "user_id = 2  # Example user ID\n",
    "combined_embedding = previous_combined_embedding  # Use previously combined embedding\n",
    "top_k = 5  # Number of recommendations\n",
    "\n",
    "recommended_items = hybrid_recommendation(\n",
    "    user_id, combined_embedding, all_embeddings, interaction_matrix, user_similarity, content_weight=0.6, collaborative_weight=0.4, top_k=top_k\n",
    ")\n",
    "\n",
    "print(f\"Recommended items for user {user_id} (hybrid): {recommended_items}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid Model using Content-Based filtering and 2nd Collaborative Filtering approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_recommendations_ncf(user_id, model, all_embeddings, combined_embedding, k=5, content_weight=0.6, cf_weight=0.4):\n",
    "    \"\"\"\n",
    "    Generate hybrid recommendations by combining content-based and collaborative filtering scores.\n",
    "    \n",
    "    Args:\n",
    "        user_id: User ID for the collaborative filtering part.\n",
    "        model: The trained NCF model for collaborative filtering.\n",
    "        all_embeddings: Precomputed item embeddings (tensor).\n",
    "        combined_embedding: Combined embedding from content-based filtering.\n",
    "        k: Number of recommendations to return.\n",
    "        content_weight: Weight for content-based filtering scores.\n",
    "        cf_weight: Weight for collaborative filtering scores.\n",
    "    \n",
    "    Returns:\n",
    "        List of tuples [(item_id, score)] for the top-k recommended items.\n",
    "    \"\"\"\n",
    "    # Collaborative Filtering Scores\n",
    "    user_id_tensor = torch.tensor([user_id], dtype=torch.long)\n",
    "    with torch.no_grad():\n",
    "        cf_scores = []\n",
    "        for item_id in range(all_embeddings.shape[0]):\n",
    "            item_embedding = all_embeddings[item_id].unsqueeze(0)\n",
    "            score = model(user_id_tensor, item_embedding)\n",
    "            cf_scores.append(score.item())\n",
    "    cf_scores = np.array(cf_scores)\n",
    "    \n",
    "    # Content-Based Filtering Scores\n",
    "    content_similarities = cosine_similarity([combined_embedding], all_embeddings.numpy())[0]\n",
    "    \n",
    "    # Hybrid Scores\n",
    "    hybrid_scores = content_weight * content_similarities + cf_weight * cf_scores\n",
    "    \n",
    "    # Top-k Recommendations\n",
    "    top_k_indices = np.argsort(hybrid_scores)[-k:][::-1]\n",
    "    top_k_items_with_scores = [(idx, hybrid_scores[idx]) for idx in top_k_indices]\n",
    "    \n",
    "    return top_k_items_with_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 hybrid recommendations for user 3:\n",
      "Item ID: 10, Hybrid Score: 0.8064\n",
      "Item ID: 60455, Hybrid Score: 0.7177\n",
      "Item ID: 97505, Hybrid Score: 0.7103\n",
      "Item ID: 27393, Hybrid Score: 0.7007\n",
      "Item ID: 75978, Hybrid Score: 0.6975\n"
     ]
    }
   ],
   "source": [
    "user_id = 3  # Example user ID\n",
    "current_embedding = all_embeddings_ncf[10]  # Example: embedding of the clicked artwork\n",
    "previous_combined_embedding = None  # Start with no previous interaction\n",
    "combined_embedding = combine_embeddings_for_recommendation(current_embedding, previous_combined_embedding, weight=0.7)\n",
    "\n",
    "top_k_recommendations = hybrid_recommendations_ncf(\n",
    "    user_id, model, all_embeddings_ncf, combined_embedding, k=5\n",
    ")\n",
    "\n",
    "print(f\"Top 5 hybrid recommendations for user {user_id}:\")\n",
    "for item_id, score in top_k_recommendations:\n",
    "    print(f\"Item ID: {item_id}, Hybrid Score: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
